{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = pd.read_csv('ActionList-Reduced.csv')\n",
    "#Setting time column from string to Float\n",
    "actions['Time'] = actions['Time'].apply(lambda x: x.replace(',','.')).astype(float)\n",
    "\n",
    "ref = pd.read_csv('RefPostos-Reduced.csv')\n",
    "ref = ref.drop(columns=['Classe Posto','Ordem Atividades','Parte'])\n",
    "#Setting time column from string to Float\n",
    "ref['Tempo (s)'] = ref['Tempo (s)'].apply(lambda x: x.replace(',','.')).astype(float)\n",
    "\n",
    "operators = pd.read_csv('Operadores-Reduced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Referencia (44 Postos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28d1bc849d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graph for referential comparison data\n",
    "ref.groupby('Posto')['Tempo (s)'].sum().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Estado Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando TT com o valor inteiro arredondado para cimada ação mais demorada da lista de ações e definindo a sequência inicial como a ordem dos operadores no arquivo de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitialTT():\n",
    "    return math.ceil(max(actions.Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getFirstOperatorsOrder():\n",
    "    \n",
    "    order = []\n",
    "    \n",
    "    for i in range(0,len(operators.Nome)):\n",
    "        order = np.append(order, i)\n",
    "        \n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStateNumber(state):\n",
    "    state_number = 0\n",
    "    \n",
    "    state_number = state_number + state[0]*6**5\n",
    "    \n",
    "    state_number = state_number + state[1]*1\n",
    "    state_number = state_number + state[2]*6**1\n",
    "    state_number = state_number + state[3]*6**2\n",
    "    state_number = state_number + state[4]*6**3\n",
    "    state_number = state_number + state[5]*6**4\n",
    "    \n",
    "    return state_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(getStateNumber([11,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "\n",
    "        total_time = state[0]\n",
    "        order = state[1:len(state)-1]\n",
    "\n",
    "        #Definindo a distribuição sequencial baseada em TT e order\n",
    "        accumulator = 0\n",
    "        full_time = 0\n",
    "        time_posto = [0] * (len(state)-1)\n",
    "        index = 0\n",
    "        action_num = 0\n",
    "\n",
    "        while(action_num < len(actions.Time)-1 and index < len(operators.Nome)):\n",
    "\n",
    "            while accumulator < state[0]-actions.Time[action_num]*operators.Eficiencia[order[index-1]]:\n",
    "                if(action_num < len(actions.Time)-1):\n",
    "                    accumulator = accumulator + actions.Time[action_num]*operators.Eficiencia[order[index-1]]\n",
    "                    action_num = action_num + 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            time_posto[index] = accumulator \n",
    "            full_time = full_time + accumulator\n",
    "            accumulator = 0\n",
    "            index = index + 1\n",
    "\n",
    "        missing_actions = len(actions.Time)-1 - action_num \n",
    "        not_working_operators = len(operators.Nome) - np.count_nonzero(time_posto)\n",
    "        mean_time = time_posto[time_posto!=0].mean()\n",
    "        max_time = np.max(time_posto)\n",
    "\n",
    "        if(missing_actions > 0 or not_working_operators > 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enums import *\n",
    "import random\n",
    "\n",
    "class LayoutSimulation:\n",
    "    \n",
    "    #def __init__(self, tt=np.max(actions.Time),order = getFirstOperatorsOrder()):   \n",
    "\n",
    "    def take_action(self, action):\n",
    "        if action == 'INCREASE':\n",
    "            self.state[0] = self.state[0] + 1\n",
    "            reward = get_reward(self.state)\n",
    "        elif action == 'DECREASE':\n",
    "            self.state[0] = self.state[0] - 1\n",
    "            reward = get_reward(self.state)\n",
    "        else:\n",
    "            aux = self.state[1]\n",
    "            self.state[1] = self.state[action]\n",
    "            self.state[action] = aux\n",
    "            reward = get_reward(self.state)\n",
    "            \n",
    "        return self.state, reward\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset state to zero, the beginning of the dungeon\n",
    "        self.state = np.concatenate(([getInitialTT()],getFirstOperatorsOrder()))\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from enums import *\n",
    "import random\n",
    "import math\n",
    "\n",
    "class Supervisor:\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, exploration_rate=1.0, iterations=10000):\n",
    "        #self.q_table = [ [ 0 for i in range(100*math.factorial(len(operators.Nome))) ] for j in range(len(operators.Nome)+1)] # Spreadsheet (Q-table) for rewards accounting\n",
    "        self.q_table = [ [ 0 for i in range(785376) ] for j in range(len(operators.Nome)+1)]\n",
    "        self.learning_rate = learning_rate # How much we appreciate new q-value over current\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = 1.0 # Initial exploration rate\n",
    "        self.exploration_delta = 1.0 / iterations # Shift from exploration to exploitation\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        \n",
    "        # Choose best rewarding action to take\n",
    "        best_action = numpy.max(max(self.q_table[state])) \n",
    "        \n",
    "        if best_action == 0:\n",
    "            return 'INCREASE'\n",
    "        if best_action == 1:\n",
    "            return 'DECREASE'\n",
    "        else:\n",
    "            return best_action\n",
    "\n",
    "    def random_action(self):\n",
    "        rand = random.randint(0, len(operators.Nome) + 1)\n",
    "        if(rand == 0):\n",
    "            return 'INCREASE'\n",
    "        if(rand == 1):\n",
    "            return 'DECREASE'\n",
    "        else:\n",
    "            return rand\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        # Old Q-table value\n",
    "        \n",
    "        print(f'{action} , {getStateNumber(old_state)}')\n",
    "        old_value = self.q_table[action][getStateNumber(old_state)]\n",
    "        # What would be our best next action?\n",
    "        future_action = self.greedy_action(new_state)\n",
    "        # What is reward for the best next action?\n",
    "        future_reward = self.q_table[future_action][getStateNumber(new_state)]\n",
    "\n",
    "        # Main Q-table updating algorithm\n",
    "        new_value = old_value + self.learning_rate * (reward + self.discount * future_reward - old_value)\n",
    "        self.q_table[action][getStateNumber(old_state)] = new_value\n",
    "\n",
    "        # Finally shift our exploration_rate toward zero (less gambling)\n",
    "        if self.exploration_rate > 0:\n",
    "            self.exploration_rate -= self.exploration_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "def SimulationStart():\n",
    "    # parse arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1, help='How quickly the algorithm tries to learn')\n",
    "    parser.add_argument('--discount', type=float, default=0.95, help='Discount for estimated future action')\n",
    "    parser.add_argument('--iterations', type=int, default=2000, help='Iteration count')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    agent = Supervisor()\n",
    "\n",
    "    # setup simulation\n",
    "    line_layout = LayoutSimulation()\n",
    "    tt = getInitialTT()\n",
    "    order = getFirstOperatorsOrder()\n",
    "    line_layout.reset()\n",
    "    total_reward = 0 # Score keeping\n",
    "\n",
    "    print(f'Estado Inicial: {line_layout.state}')\n",
    "    print('\\n')\n",
    "    #Main loop\n",
    "    for step in range(FLAGS.iterations):\n",
    "        old_state = line_layout.state # Store current state\n",
    "        action = agent.get_next_action(old_state) # Query agent for the next action\n",
    "        print(f'Agente executou ação: {action}')\n",
    "        new_state, reward = line_layout.take_action(action) # Take action, get new state and reward\n",
    "        print(f'Novo Estado: {new_state}')\n",
    "        print(f'Recompensa: {reward}')\n",
    "        print('\\n')\n",
    "        agent.update(old_state, new_state, action, reward) # Let the agent update internals\n",
    "\n",
    "        total_reward += reward # Keep score\n",
    "        if step % 250 == 0: # Print out metadata every 100th iteration\n",
    "            print(json.dumps({'step': step, 'total_reward': total_reward}))\n",
    "\n",
    "        time.sleep(0.0001) # Avoid spamming stdout too fast!\n",
    "\n",
    "    print(\"Final Q-table\", agent.q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iniciar Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado Inicial: [18.  0.  1.  2.  3.  4.]\n",
      "\n",
      "\n",
      "Agente executou ação: 3\n",
      "Novo Estado: [18.  2.  1.  0.  3.  4.]\n",
      "Recompensa: 0\n",
      "\n",
      "\n",
      "3 , 145808.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-356-e4373f5ab8bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSimulationStart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-129-15d7128541da>\u001b[0m in \u001b[0;36mSimulationStart\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Recompensa: {reward}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Let the agent update internals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;31m# Keep score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-355-efc1e78b9dd0>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, old_state, new_state, action, reward)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{action} , {getStateNumber(old_state)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mold_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgetStateNumber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;31m# What would be our best next action?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mfuture_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.float64"
     ]
    }
   ],
   "source": [
    "SimulationStart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
